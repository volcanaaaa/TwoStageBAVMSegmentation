{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe141c66-8d58-4b72-a561-beb576bdd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "# ==================== 第一阶段：2D U-Net检测模型 ====================\n",
    "def kaiming_normal_init_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "def sparse_init_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.sparse_(m.weight, sparsity=0.1)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "class ConvBlock2D(nn.Module):\n",
    "    \"\"\"two convolution layers with batch norm and leaky relu\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dropout_p):\n",
    "        super(ConvBlock2D, self).__init__()\n",
    "        self.conv_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_conv(x)\n",
    "\n",
    "class DownBlock2D(nn.Module):\n",
    "    \"\"\"Downsampling followed by ConvBlock\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dropout_p):\n",
    "        super(DownBlock2D, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock2D(in_channels, out_channels, dropout_p)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class UpBlock2D(nn.Module):\n",
    "    \"\"\"Upssampling followed by ConvBlock\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels1, in_channels2, out_channels, dropout_p,\n",
    "                 bilinear=False):\n",
    "        super(UpBlock2D, self).__init__()\n",
    "        self.bilinear = bilinear\n",
    "        if bilinear:\n",
    "            self.conv1x1 = nn.Conv2d(in_channels1, in_channels2, kernel_size=1)\n",
    "            self.up = nn.Upsample(\n",
    "                scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels1, in_channels2, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock2D(in_channels2 * 2, out_channels, dropout_p)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        if self.bilinear:\n",
    "            x1 = self.conv1x1(x1)\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class Encoder2D(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Encoder2D, self).__init__()\n",
    "        self.params = params\n",
    "        self.in_chns = self.params['in_chns']\n",
    "        self.ft_chns = self.params['feature_chns']\n",
    "        self.n_class = self.params['class_num']\n",
    "        self.bilinear = self.params['bilinear']\n",
    "        self.dropout = self.params['dropout']\n",
    "        assert (len(self.ft_chns) == 5)\n",
    "        self.in_conv = ConvBlock2D(\n",
    "            self.in_chns, self.ft_chns[0], self.dropout[0])\n",
    "        self.down1 = DownBlock2D(\n",
    "            self.ft_chns[0], self.ft_chns[1], self.dropout[1])\n",
    "        self.down2 = DownBlock2D(\n",
    "            self.ft_chns[1], self.ft_chns[2], self.dropout[2])\n",
    "        self.down3 = DownBlock2D(\n",
    "            self.ft_chns[2], self.ft_chns[3], self.dropout[3])\n",
    "        self.down4 = DownBlock2D(\n",
    "            self.ft_chns[3], self.ft_chns[4], self.dropout[4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.in_conv(x)\n",
    "        x1 = self.down1(x0)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        return [x0, x1, x2, x3, x4]\n",
    "\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Decoder2D, self).__init__()\n",
    "        self.params = params\n",
    "        self.in_chns = self.params['in_chns']\n",
    "        self.ft_chns = self.params['feature_chns']\n",
    "        self.n_class = self.params['class_num']\n",
    "        self.bilinear = self.params['bilinear']\n",
    "        assert (len(self.ft_chns) == 5)\n",
    "\n",
    "        self.up1 = UpBlock2D(\n",
    "            self.ft_chns[4], self.ft_chns[3], self.ft_chns[3], dropout_p=0.0)\n",
    "        self.up2 = UpBlock2D(\n",
    "            self.ft_chns[3], self.ft_chns[2], self.ft_chns[2], dropout_p=0.0)\n",
    "        self.up3 = UpBlock2D(\n",
    "            self.ft_chns[2], self.ft_chns[1], self.ft_chns[1], dropout_p=0.0)\n",
    "        self.up4 = UpBlock2D(\n",
    "            self.ft_chns[1], self.ft_chns[0], self.ft_chns[0], dropout_p=0.0)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(self.ft_chns[0], self.n_class,\n",
    "                                  kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        x0 = feature[0]\n",
    "        x1 = feature[1]\n",
    "        x2 = feature[2]\n",
    "        x3 = feature[3]\n",
    "        x4 = feature[4]\n",
    "\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        x = self.up4(x, x0)\n",
    "        output = self.out_conv(x)\n",
    "        return output\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_chns, class_num):\n",
    "        super(UNet2D, self).__init__()\n",
    "\n",
    "        params = {'in_chns': in_chns,\n",
    "                  'feature_chns': [16, 32, 64, 128, 256],\n",
    "                  'dropout': [0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "                  'class_num': class_num,\n",
    "                  'bilinear': False,\n",
    "                  'acti_func': 'relu'}\n",
    "\n",
    "        self.encoder = Encoder2D(params)\n",
    "        self.decoder = Decoder2D(params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        output = self.decoder(feature)\n",
    "        return output\n",
    "\n",
    "# ==================== 第二阶段：3D自注意力网络 ====================\n",
    "class ChannelAttentionModule3D(nn.Module):\n",
    "    def __init__(self, channel, ratio=16):\n",
    "        super(ChannelAttentionModule3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool3d(1)\n",
    "\n",
    "        self.shared_MLP = nn.Sequential(\n",
    "            nn.Conv3d(channel, channel // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(channel // ratio, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = self.shared_MLP(self.avg_pool(x))\n",
    "        maxout = self.shared_MLP(self.max_pool(x))\n",
    "        attention = self.sigmoid(avgout + maxout)\n",
    "        return attention, attention * x\n",
    "\n",
    "class SpatialAttentionModule3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttentionModule3D, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avgout, maxout], dim=1)\n",
    "        attention = self.sigmoid(self.conv3d(out))\n",
    "        return attention, attention * x\n",
    "\n",
    "class CBAM3D(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CBAM3D, self).__init__()\n",
    "        self.channel_attention = ChannelAttentionModule3D(channel)\n",
    "        self.spatial_attention = SpatialAttentionModule3D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ca_att, ca_out = self.channel_attention(x)\n",
    "        sa_att, sa_out = self.spatial_attention(ca_out)\n",
    "        return sa_out, ca_att, sa_att\n",
    "\n",
    "class conv_block_3d(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_3d, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv_3d(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_conv_3d, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True),\n",
    "            nn.Conv3d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm3d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class UNet_CBAM_3D(nn.Module):\n",
    "    def __init__(self, in_chans=1, num_classes=4):\n",
    "        super(UNet_CBAM_3D, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block_3d(ch_in=in_chans, ch_out=16)\n",
    "        self.Conv2 = conv_block_3d(ch_in=16, ch_out=32)\n",
    "        self.Conv3 = conv_block_3d(ch_in=32, ch_out=64)\n",
    "        self.Conv4 = conv_block_3d(ch_in=64, ch_out=128)\n",
    "        self.Conv5 = conv_block_3d(ch_in=128, ch_out=256)\n",
    "\n",
    "        self.cbam1 = CBAM3D(channel=16)\n",
    "        self.cbam2 = CBAM3D(channel=32)\n",
    "        self.cbam3 = CBAM3D(channel=64)\n",
    "        self.cbam4 = CBAM3D(channel=128)\n",
    "\n",
    "        self.Up5 = up_conv_3d(ch_in=256, ch_out=128)\n",
    "        self.Up_conv5 = conv_block_3d(ch_in=256, ch_out=128)\n",
    "\n",
    "        self.Up4 = up_conv_3d(ch_in=128, ch_out=64)\n",
    "        self.Up_conv4 = conv_block_3d(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Up3 = up_conv_3d(ch_in=64, ch_out=32)\n",
    "        self.Up_conv3 = conv_block_3d(ch_in=64, ch_out=32)\n",
    "\n",
    "        self.Up2 = up_conv_3d(ch_in=32, ch_out=16)\n",
    "        self.Up_conv2 = conv_block_3d(ch_in=32, ch_out=16)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv3d(16, num_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "        x1, ca1, sa1 = self.cbam1(x1)\n",
    "        \n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        x2, ca2, sa2 = self.cbam2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "        x3, ca3, sa3 = self.cbam3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "        x4, ca4, sa4 = self.cbam4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1, (ca1, sa1, ca2, sa2, ca3, sa3, ca4, sa4)\n",
    "\n",
    "# ==================== 两阶段集成模型 ====================\n",
    "class TwoStageBAVMSegmentation(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=4):\n",
    "        super(TwoStageBAVMSegmentation, self).__init__()\n",
    "        # 第一阶段：2D U-Net检测模型\n",
    "        self.stage1_detector = UNet2D(in_chns=in_channels, class_num=1)  # 二分类：bAVMs或背景\n",
    "        \n",
    "        # 第二阶段：3D自注意力网络\n",
    "        self.stage2_segmentor = UNet_CBAM_3D(in_chans=in_channels, num_classes=num_classes)\n",
    "        \n",
    "        # ROI大小\n",
    "        self.roi_size = 128\n",
    "        \n",
    "    def calculate_mass_center(self, mask):\n",
    "        \"\"\"计算二值掩码的质量中心\"\"\"\n",
    "        # 将掩码转换为numpy数组\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.detach().cpu().numpy()\n",
    "        \n",
    "        # 找到非零元素的坐标\n",
    "        coords = np.argwhere(mask > 0)\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            # 如果没有检测到任何bAVMs，返回图像中心\n",
    "            return np.array(mask.shape) // 2\n",
    "        \n",
    "        # 计算质量中心\n",
    "        mass_center = coords.mean(axis=0)\n",
    "        return mass_center.astype(int)\n",
    "    \n",
    "    def extract_roi(self, volume, mass_center):\n",
    "        \"\"\"根据质量中心提取ROI\"\"\"\n",
    "        d, h, w = volume.shape[-3:]\n",
    "        roi_size = self.roi_size\n",
    "        \n",
    "        # 计算ROI的起始和结束坐标\n",
    "        start_d = max(0, mass_center[0] - roi_size // 2)\n",
    "        end_d = min(d, mass_center[0] + roi_size // 2)\n",
    "        start_h = max(0, mass_center[1] - roi_size // 2)\n",
    "        end_h = min(h, mass_center[1] + roi_size // 2)\n",
    "        start_w = max(0, mass_center[2] - roi_size // 2)\n",
    "        end_w = min(w, mass_center[2] + roi_size // 2)\n",
    "        \n",
    "        # 提取ROI\n",
    "        roi = volume[..., start_d:end_d, start_h:end_h, start_w:end_w]\n",
    "        \n",
    "        # 如果需要，进行填充以确保ROI大小一致\n",
    "        pad_d = roi_size - (end_d - start_d)\n",
    "        pad_h = roi_size - (end_h - start_h)\n",
    "        pad_w = roi_size - (end_w - start_w)\n",
    "        \n",
    "        if any([pad_d > 0, pad_h > 0, pad_w > 0]):\n",
    "            roi = nn.functional.pad(roi, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "        \n",
    "        return roi, (start_d, end_d, start_h, end_h, start_w, end_w)\n",
    "    \n",
    "    def forward(self, x, phase='test'):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        - x: 输入的多模态图像 [B, C, D, H, W]\n",
    "        - phase: 训练阶段 ('train_stage1', 'train_stage2') 或测试阶段 ('test')\n",
    "        \"\"\"\n",
    "        if phase == 'train_stage1':\n",
    "            # 第一阶段训练：处理2D切片\n",
    "            b, c, d, h, w = x.shape\n",
    "            # 将3D体积重塑为2D切片批次\n",
    "            x_2d = x.permute(0, 2, 1, 3, 4).contiguous().view(b * d, c, h, w)\n",
    "            # 通过第一阶段网络\n",
    "            stage1_output = self.stage1_detector(x_2d)\n",
    "            # 重塑回3D形状\n",
    "            stage1_output = stage1_output.view(b, d, 1, h, w).permute(0, 2, 1, 3, 4)\n",
    "            return stage1_output\n",
    "            \n",
    "        elif phase == 'train_stage2':\n",
    "            # 第二阶段训练：直接使用裁剪的ROI\n",
    "            return self.stage2_segmentor(x)\n",
    "            \n",
    "        else:  # 测试阶段\n",
    "            # 第一阶段：检测bAVMs位置\n",
    "            b, c, d, h, w = x.shape\n",
    "            x_2d = x.permute(0, 2, 1, 3, 4).contiguous().view(b * d, c, h, w)\n",
    "            stage1_output = self.stage1_detector(x_2d)\n",
    "            stage1_mask = (torch.sigmoid(stage1_output) > 0.5).float()\n",
    "            stage1_mask = stage1_mask.view(b, d, 1, h, w).permute(0, 2, 1, 3, 4)\n",
    "            \n",
    "            # 计算质量中心并提取ROI\n",
    "            roi_coords_list = []\n",
    "            roi_list = []\n",
    "            \n",
    "            for i in range(b):\n",
    "                mass_center = self.calculate_mass_center(stage1_mask[i, 0])\n",
    "                roi, coords = self.extract_roi(x[i], mass_center)\n",
    "                roi_list.append(roi)\n",
    "                roi_coords_list.append(coords)\n",
    "            \n",
    "            roi_batch = torch.stack(roi_list, dim=0)\n",
    "            \n",
    "            # 第二阶段：在ROI内进行精细分割\n",
    "            stage2_output, attentions = self.stage2_segmentor(roi_batch)\n",
    "            \n",
    "            # 将分割结果映射回原始图像空间\n",
    "            full_size_output = torch.zeros_like(x[:, :1, :, :, :])\n",
    "            for i in range(b):\n",
    "                start_d, end_d, start_h, end_h, start_w, end_w = roi_coords_list[i]\n",
    "                roi_d, roi_h, roi_w = end_d - start_d, end_h - start_h, end_w - start_w\n",
    "                \n",
    "                # 调整分割结果大小以匹配原始ROI大小\n",
    "                resized_output = nn.functional.interpolate(\n",
    "                    stage2_output[i:i+1], \n",
    "                    size=(roi_d, roi_h, roi_w), \n",
    "                    mode='trilinear', \n",
    "                    align_corners=True\n",
    "                )\n",
    "                \n",
    "                # 将结果放回原始图像中的正确位置\n",
    "                full_size_output[i, 0, start_d:end_d, start_h:end_h, start_w:end_w] = resized_output[0, 0]\n",
    "            \n",
    "            return full_size_output, stage1_mask, roi_coords_list\n",
    "\n",
    "# ==================== 辅助函数 ====================\n",
    "def FeatureDropout(x):\n",
    "    attention = torch.mean(x, dim=1, keepdim=True)\n",
    "    max_val, _ = torch.max(attention.view(\n",
    "        x.size(0), -1), dim=1, keepdim=True)\n",
    "    threshold = max_val * np.random.uniform(0.7, 0.9)\n",
    "    threshold = threshold.view(x.size(0), 1, 1, 1).expand_as(attention)\n",
    "    drop_mask = (attention < threshold).float()\n",
    "    x = x.mul(drop_mask)\n",
    "    return x\n",
    "\n",
    "class FeatureNoise(nn.Module):\n",
    "    def __init__(self, uniform_range=0.3):\n",
    "        super(FeatureNoise, self).__init__()\n",
    "        self.uni_dist = Uniform(-uniform_range, uniform_range)\n",
    "\n",
    "    def feature_based_noise(self, x):\n",
    "        noise_vector = self.uni_dist.sample(\n",
    "            x.shape[1:]).to(x.device).unsqueeze(0)\n",
    "        x_noise = x.mul(noise_vector) + x\n",
    "        return x_noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_based_noise(x)\n",
    "        return x\n",
    "\n",
    "# ==================== 测试代码 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = TwoStageBAVMSegmentation(in_channels=3, num_classes=4)\n",
    "    \n",
    "    # 测试第一阶段\n",
    "    print(\"Testing stage 1...\")\n",
    "    test_input = torch.rand(1, 3, 32, 256, 256)  # 模拟多模态3D输入\n",
    "    stage1_output = model(test_input, phase='train_stage1')\n",
    "    print(f\"Stage 1 output shape: {stage1_output.shape}\")\n",
    "    \n",
    "    # 测试第二阶段\n",
    "    print(\"Testing stage 2...\")\n",
    "    roi_input = torch.rand(1, 3, 128, 128, 128)  # 模拟ROI输入\n",
    "    stage2_output, attentions = model(roi_input, phase='train_stage2')\n",
    "    print(f\"Stage 2 output shape: {stage2_output.shape}\")\n",
    "    \n",
    "    # 测试完整流程\n",
    "    print(\"Testing full pipeline...\")\n",
    "    full_output, coarse_mask, roi_coords = model(test_input, phase='test')\n",
    "    print(f\"Full output shape: {full_output.shape}\")\n",
    "    print(f\"Coarse mask shape: {coarse_mask.shape}\")\n",
    "    print(f\"ROI coordinates: {roi_coords}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
